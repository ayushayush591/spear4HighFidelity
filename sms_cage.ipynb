{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***End to End tutorial for SMS_SPAM labeling using Cage:***\n",
    "**The paper, documentation, colab notebook can be found here:** [Paper](https://ojs.aaai.org/index.php/AAAI/article/view/5742), [Documentation](https://spear-decile.readthedocs.io/en/latest/#cage), [Colab](https://colab.research.google.com/drive/1vec-Q-xO9wQtM3p_CZ7237gCq0xIR9b9?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "User don't need to include this cell to use the package\n",
    "'''\n",
    "import sys\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Defining an Enum to hold labels:***\n",
    "### **Representation of class Labels**\n",
    "\n",
    "<p>All the class labels for which we define labeling functions are encoded in enum and utilized in our next tasks. Make sure not to define an Abstain(Labeling function(LF) not deciding anything) class inside this Enum, instead import the ABSTAIN object as used later in LF section.</p>\n",
    "\n",
    "<p>SPAM dataset contains 2 classes i.e <b>HAM</b> and <b>SPAM</b>. Note that the numbers we associate can be anything but it is suggested to use a continuous numbers from 0 to number_of_classes-1</p>\n",
    "\n",
    "<p><b>**Note that even though this example is a binary classification, this(SPEAR) library supports multi-class classification**</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "# enum to hold the class labels\n",
    "class ClassLabels(enum.Enum):\n",
    "    SPAM = 1\n",
    "    HAM = 0\n",
    "\n",
    "THRESHOLD = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Defining preprocessors, continuous_scorers, labeling functions:***\n",
    "During labeling the unlabelled data we lookup for few keywords to assign a class SMS.\n",
    "\n",
    "<b>Example</b> : *If a message contains apply or buy in it then most probably the message is spam*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigWord1 = {\"free\",\"credit\",\"cheap\",\"apply\",\"buy\",\"attention\",\"shop\",\"sex\",\"soon\",\"now\",\"spam\"}\n",
    "trigWord2 = {\"gift\",\"click\",\"new\",\"online\",\"discount\",\"earn\",\"miss\",\"hesitate\",\"exclusive\",\"urgent\"}\n",
    "trigWord3 = {\"cash\",\"refund\",\"insurance\",\"money\",\"guaranteed\",\"save\",\"win\",\"teen\",\"weight\",\"hair\"}\n",
    "notFreeWords = {\"toll\",\"Toll\",\"freely\",\"call\",\"meet\",\"talk\",\"feedback\"}\n",
    "notFreeSubstring = {\"not free\",\"you are\",\"when\",\"wen\"}\n",
    "firstAndSecondPersonWords = {\"I\",\"i\",\"u\",\"you\",\"ur\",\"your\",\"our\",\"we\",\"us\",\"youre\"}\n",
    "thirdPersonWords = {\"He\",\"he\",\"She\",\"she\",\"they\",\"They\",\"Them\",\"them\",\"their\",\"Their\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Declaration of a simple preprocessor function**\n",
    "\n",
    "\n",
    "For most of the tasks in NLP, computer vivsion instead of using the raw datapoint we preprocess the datapoint and then label it. Preprocessor functions are used to preprocess an instance before labeling it. We use **`@preprocessor(name,resources)`** decorator to declare a function as preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spear.labeling import preprocessor\n",
    "\n",
    "\n",
    "@preprocessor(name = \"LOWER_CASE\")\n",
    "def convert_to_lower(x):\n",
    "    return x.lower().strip()\n",
    "\n",
    "lower = convert_to_lower(\"RED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Some Labeling function(LF) definitions**\n",
    "Below are some examples on how to define LFs and continuous LFs(CLFs). To get the continuous score for a CLF, we need to define a function with continuous_scorer decorator(just like labeling_function decorator) and pass it to a CLF as displayed below. Also note how the continuous score can be used in CLF. Note that the word_similarity is the function with continuous_scorer decorator and is written in con_scorer file(this file is not a part of package) in same folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loading\n",
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "from spear.labeling import labeling_function, ABSTAIN\n",
    "\n",
    "from helper.con_scorer import word_similarity\n",
    "import re\n",
    "\n",
    "\n",
    "@preprocessor()\n",
    "def convert_to_lower(x):\n",
    "    return x.lower().strip()\n",
    "\n",
    "\n",
    "@labeling_function(resources=dict(keywords=trigWord1),pre=[convert_to_lower],label=ClassLabels.SPAM)\n",
    "def LF1(c,**kwargs):    \n",
    "    if len(kwargs[\"keywords\"].intersection(c.split())) > 0:\n",
    "        return ClassLabels.SPAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(resources=dict(keywords=trigWord2),pre=[convert_to_lower],label=ClassLabels.SPAM)\n",
    "def LF2(c,**kwargs):\n",
    "    if len(kwargs[\"keywords\"].intersection(c.split())) > 0:\n",
    "        return ClassLabels.SPAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(resources=dict(keywords=trigWord3),pre=[convert_to_lower],label=ClassLabels.SPAM)\n",
    "def LF3(c,**kwargs):\n",
    "    if len(kwargs[\"keywords\"].intersection(c.split())) > 0:\n",
    "        return ClassLabels.SPAM \n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(resources=dict(keywords=notFreeWords),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def LF4(c,**kwargs):\n",
    "    if \"free\" in c.split() and len(kwargs[\"keywords\"].intersection(c.split()))>0:\n",
    "        return ClassLabels.HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(resources=dict(keywords=notFreeSubstring),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def LF5(c,**kwargs):\n",
    "    for pattern in kwargs[\"keywords\"]:    \n",
    "        if \"free\" in c.split() and re.search(pattern,c, flags= re.I):\n",
    "            return ClassLabels.HAM\n",
    "    return ABSTAIN\n",
    "\n",
    "@labeling_function(resources=dict(keywords=firstAndSecondPersonWords),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def LF6(c,**kwargs):\n",
    "    if \"free\" in c.split() and len(kwargs[\"keywords\"].intersection(c.split()))>0:\n",
    "        return ClassLabels.HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "\n",
    "@labeling_function(resources=dict(keywords=thirdPersonWords),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def LF7(c,**kwargs):\n",
    "    if \"free\" in c.split() and len(kwargs[\"keywords\"].intersection(c.split()))>0:\n",
    "        return ClassLabels.HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(label=ClassLabels.SPAM)\n",
    "def LF8(c,**kwargs):\n",
    "    if (sum(1 for ch in c if ch.isupper()) > 6):\n",
    "        return ClassLabels.SPAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=word_similarity,resources=dict(keywords=trigWord1),pre=[convert_to_lower],label=ClassLabels.SPAM)\n",
    "def CLF1(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.SPAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=word_similarity,resources=dict(keywords=trigWord2),pre=[convert_to_lower],label=ClassLabels.SPAM)\n",
    "def CLF2(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.SPAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=word_similarity,resources=dict(keywords=trigWord3),pre=[convert_to_lower],label=ClassLabels.SPAM)\n",
    "def CLF3(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.SPAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=word_similarity,resources=dict(keywords=notFreeWords),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def CLF4(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=word_similarity,resources=dict(keywords=notFreeSubstring),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def CLF5(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=word_similarity,resources=dict(keywords=firstAndSecondPersonWords),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def CLF6(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=word_similarity,resources=dict(keywords=thirdPersonWords),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def CLF7(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=lambda x: 1-np.exp(float(-(sum(1 for ch in x if ch.isupper()))/2)),label=ClassLabels.SPAM)\n",
    "def CLF8(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.SPAM\n",
    "    else:\n",
    "        return ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Accumulating all LFs into rules, an LFset(a class) object:***\n",
    "### **Importing LFSet and passing LFs we defined, to that class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spear.labeling import LFSet\n",
    "\n",
    "LFS = [LF1,\n",
    "    LF2,\n",
    "    LF3,\n",
    "    LF4,\n",
    "    LF5,\n",
    "    LF6,\n",
    "    LF7,\n",
    "    LF8,\n",
    "    CLF1,\n",
    "    CLF2,\n",
    "    CLF3,\n",
    "    CLF4,\n",
    "    CLF5,\n",
    "    CLF6,\n",
    "    CLF7,\n",
    "    CLF8\n",
    "      ]\n",
    "\n",
    "rules = LFSet(\"SPAM_LF\")\n",
    "rules.add_lf_list(LFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Loading data:***\n",
    "### **Load the data: X, Y**\n",
    "<p>Note that the utils below is not a part of package but is used to load the necessary data. User have to use some means(which doesn't matter) to load his data(X, Y). X is the raw data that is to be passed to LFs and Y are true labels(if available). Note that feature matrix is not needed in Cage algorithm but it is needed in JL algorithm.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.utils import load_data_to_numpy, get_test_U_data\n",
    "\n",
    "X, _, Y = load_data_to_numpy()\n",
    "\n",
    "test_size = 400\n",
    "U_size = 4500\n",
    "n_lfs = len(rules.get_lfs())\n",
    "\n",
    "X_T, Y_T, _, X_U, _= get_test_U_data(X, Y, n_lfs, test_size, U_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Labeling data:***\n",
    "### **Paths**\n",
    "* path_json: path to json file generated by PreLabels\n",
    "* T_path_pkl: path to pkl file generated by PreLabels containing the test data with true labels\n",
    "* U_path_pkl: path to pkl file generated by PreLabels containing the unlabelled data without true labels\n",
    "* log_path: path to save the log which is generated during the algorithm\n",
    "* params_path: path to save parameters of model\n",
    "\n",
    "<p>Make sure that the directory of the files(in above paths) exists. Note that any existing contents in pickle files will be erased.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_json = 'data_pipeline/Cage/sms_json.json'\n",
    "T_path_pkl = 'data_pipeline/Cage/sms_pickle_T.pkl' #test data - have true labels\n",
    "U_path_pkl = 'data_pipeline/Cage/sms_pickle_U.pkl' #unlabelled data - don't have true labels\n",
    "\n",
    "log_path_cage_1 = 'log/Cage/sms_log_1.txt' #cage is an algorithm, can be found below\n",
    "params_path = 'params/Cage/sms_params.pkl' #file path to store parameters of Cage, used below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing PreLabels class and using it to label data**\n",
    "Json file should be generated only once as shown below.\n",
    "<p><b>Note:</b> We don't pass feature matrix as the CAGE algorithm don't need one. Also note that we don't pass gold_lables(or true labels) to the 2nd PreLabels class which generates labels to unlabelled data(U).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:40<00:00,  9.99it/s]\n",
      "100%|██████████| 4500/4500 [07:36<00:00,  9.85it/s]\n"
     ]
    }
   ],
   "source": [
    "from spear.labeling import PreLabels\n",
    "\n",
    "sms_noisy_labels = PreLabels(name=\"sms\",\n",
    "                               data=X_T,\n",
    "                               gold_labels=Y_T,\n",
    "                               rules=rules,\n",
    "                               labels_enum=ClassLabels,\n",
    "                               num_classes=2)\n",
    "sms_noisy_labels.generate_pickle(T_path_pkl)\n",
    "sms_noisy_labels.generate_json(path_json) #generating json files once is enough\n",
    "\n",
    "sms_noisy_labels = PreLabels(name=\"sms\",\n",
    "                               data=X_U,\n",
    "                               rules=rules,\n",
    "                               labels_enum=ClassLabels,\n",
    "                               num_classes=2) #note that we don't pass gold_labels here, for the unlabelled data\n",
    "sms_noisy_labels.generate_pickle(U_path_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Accessing labeled data:***\n",
    "### **Importing and the use of get_data and get_classes**\n",
    "<p>These functions can be used to extract data from pickle files and json file respectively. Note that these are the files generated using PreLabels.</p>\n",
    "<p>For detailed contents of output, please refer documentation.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in data list:  10\n",
      "Shape of feature matrix:  (0,)\n",
      "Shape of labels matrix:  (4500, 16)\n",
      "Shape of continuous scores matrix :  (4500, 16)\n",
      "Total number of classes:  2\n",
      "Classes dictionary in json file(modified to have integer keys):  {1: 'SPAM', 0: 'HAM'}\n"
     ]
    }
   ],
   "source": [
    "from spear.utils import get_data, get_classes\n",
    "\n",
    "data_U = get_data(path = U_path_pkl, check_shapes=True)\n",
    "#check_shapes being True(above), asserts for relative shapes of arrays in pickle file\n",
    "print(\"Number of elements in data list: \", len(data_U))\n",
    "print(\"Shape of feature matrix: \", data_U[0].shape)\n",
    "print(\"Shape of labels matrix: \", data_U[1].shape)\n",
    "print(\"Shape of continuous scores matrix : \", data_U[6].shape)\n",
    "print(\"Total number of classes: \", data_U[9])\n",
    "\n",
    "classes = get_classes(path = path_json)\n",
    "print(\"Classes dictionary in json file(modified to have integer keys): \", classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Cage Algorithm:***\n",
    "### **Importing Cage class (the algorithm) and declaring an object of it**\n",
    "Cage algorithm needs only the pickle file(with labels given by LFs using PreLabels class) with unlabelled data(the data without true/gold labels) and it will predict the labels of this data. An optinal test data(which has true/gold labels) can also passed to get a log information of accuracies. \n",
    "<p><b>Note:</b> Multiple calls to fit_* functions will train parameters continuously ie, parameters are not reinitialised in fit_* functions. So, to train large data, one can call fit_* functions repeatedly on smaller chunks. Also, in order to perform multiple runs over the algorithm, one need to reinitialise paramters(by creating an object of Cage) at the start of each run.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spear.cage import Cage\n",
    "\n",
    "cage = Cage(path_json = path_json, n_lfs = n_lfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **fit_and_predict_proba function of Cage class**\n",
    "The output(probs) is a numpy matrix of shape (num_instances, num_classes) having the probability of a particular instance being that class. \n",
    "<p>Here the order of classes along a row for any instance is the ascending order of values used in enum defined before to hold labels.</p>\n",
    "<p>For more details about arguments, please refer documentation; same should be the case for any of the member functions used from here on.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:15<00:00, 12.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_test_accuracy_score: 0.7975\n",
      "test_average_metric: binary\tfinal_test_f1_score: 0.5030674846625766\n",
      "probs shape:  (4500, 2)\n",
      "labels shape:  (4500,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cage = Cage(path_json = path_json, n_lfs = n_lfs)\n",
    "\n",
    "probs = cage.fit_and_predict_proba(path_pkl = U_path_pkl, path_test = T_path_pkl, path_log = log_path_cage_1, \\\n",
    "                                   qt = 0.9, qc = 0.85, metric_avg = ['binary'], n_epochs = 200, lr = 0.01)\n",
    "labels = np.argmax(probs, 1)\n",
    "print(\"probs shape: \", probs.shape)\n",
    "print(\"labels shape: \",labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **fit_and_predict function of Cage class**\n",
    "The output(probs) is a numpy matrix of shape (num_instances,) containing integers(because need_strings is False), having the classes of each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:25<00:00,  7.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_test_accuracy_score: 0.7975\n",
      "test_average_metric: binary\tfinal_test_f1_score: 0.5030674846625766\n",
      "labels shape:  (4500,)\n",
      "<class 'numpy.int64'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cage = Cage(path_json = path_json, n_lfs = n_lfs)\n",
    "\n",
    "labels = cage.fit_and_predict(path_pkl = U_path_pkl, path_test = T_path_pkl, path_log = log_path_cage_1, \\\n",
    "                              qt = 0.9, qc = 0.85, metric_avg = ['binary'], n_epochs = 200, lr = 0.01, \\\n",
    "                              need_strings = False)\n",
    "\n",
    "print(\"labels shape: \", labels.shape)\n",
    "print(type(labels[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **fit_and_predict function of Cage class**\n",
    "The output(probs) is a numpy matrix of shape (num_instances,) containing strings(because need_strings is True), having the classes of each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:21<00:00,  9.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_test_accuracy_score: 0.7975\n",
      "test_average_metric: binary\tfinal_test_f1_score: 0.5030674846625766\n",
      "labels_strings shape:  (4500,)\n",
      "<class 'numpy.str_'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cage = Cage(path_json = path_json, n_lfs = n_lfs)\n",
    "\n",
    "labels_strings = cage.fit_and_predict(path_pkl = U_path_pkl, path_test = T_path_pkl, path_log = log_path_cage_1, \\\n",
    "                              qt = 0.9, qc = 0.85, metric_avg = ['binary'], n_epochs = 200, lr = 0.01, \\\n",
    "                              need_strings = True)\n",
    "\n",
    "print(\"labels_strings shape: \", labels_strings.shape)\n",
    "print(type(labels_strings[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Save parameters**\n",
    "<p> Make sure that the directory of the save_path file exists. Note that any existing contents in pickle file will be erased.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cage.save_params(save_path = params_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cage_2 = Cage(path_json = path_json, n_lfs = n_lfs)\n",
    "cage_2.load_params(load_path = params_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **predict_proba function of Cage class**\n",
    "The output(probs_test) is a numpy matrix of shape (num_instances, num_classes) having the probability of a particular instance being that class.\n",
    "<p>Here the order of classes along a row for any instance is the ascending order of values used in enum defined before to hold labels.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Predict is used before training any paramters in Cage class. Hope you have loaded parameters.\n",
      "probs_test shape:  (400, 2)\n"
     ]
    }
   ],
   "source": [
    "probs_test = cage_2.predict_proba(path_test = T_path_pkl, qc = 0.85) \n",
    "#NEED NOT use the same test data(above) used in Cage class before.\n",
    "print(\"probs_test shape: \",probs_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **predict function of Cage class**\n",
    "The output(probs) is a numpy matrix of shape (num_instances,) containing integers(strings) if need_strings is Flase(True), having the classes of each instance. Just the use case with need_strings as False is displayed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Predict is used before training any paramters in Cage class. Hope you have loaded parameters.\n",
      "labels_test shape:  (400,)\n",
      "accuracy_score:  0.7975\n",
      "f1_score:  0.5030674846625766\n"
     ]
    }
   ],
   "source": [
    "labels_test = cage_2.predict(path_test = T_path_pkl, qc = 0.85, need_strings = False)\n",
    "print(\"labels_test shape: \", labels_test.shape)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "#Y_T is true labels of test data, type is numpy array of shape (num_instances,)\n",
    "print(\"accuracy_score: \", accuracy_score(Y_T, labels_test))\n",
    "print(\"f1_score: \", f1_score(Y_T, labels_test, average = 'binary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Converting numpy array of integers to enums**\n",
    "The below utility from spear can help convert return values of predict(obtained when need_strings is Flase) to a numpy array of enums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<enum 'ClassLabels'>\n"
     ]
    }
   ],
   "source": [
    "from spear.utils import get_enum\n",
    "\n",
    "labels_test_enum = get_enum(np_array = labels_test, enm = ClassLabels) \n",
    "#the second argument is the Enum class defined at beginning\n",
    "print(type(labels_test_enum[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
